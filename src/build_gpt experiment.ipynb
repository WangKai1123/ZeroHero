{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a miniGPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7040e64570>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part 1: 导入相关的 package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512   # 这里其实应该是文本的最大长度（ max_seq_len）\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768    # n_embd 也叫 hidden_dim, hiden_size, 这里我同时设置了和 embed_dim 一样\n",
    "    head_size: int = n_embd // n_head\n",
    "    dropout: float = 0.1\n",
    "    # # tiktoken 使用的是 GPT-2 的词表，大约有 50257 个token\n",
    "    vocab_size: int = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicExpert(nn.Module):\n",
    "    def __init__(self,feature_in,feature_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_in,feature_out)\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMOE(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out,num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(feature_in,num_experts)\n",
    "        # output shape (batch_size, num_export)\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(\n",
    "                feature_in,feature_out\n",
    "                )for _ in range(num_experts)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        # x shape is(batch, feature_in)\n",
    "        expert_weights = self.gate(x)\n",
    "        expert_out_list = [\n",
    "            expert(x) for expert in self.experts\n",
    "        ] # 每一个expert输出一个（batch, feature_out)\n",
    "        \n",
    "        expert_outputs = [\n",
    "            expert_out.unsqueeze(1)\n",
    "            for expert_out in expert_out_list\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        # expert out 是(b,1,feature_out)\n",
    "        expert_output = torch.concat(\n",
    "            expert_outputs,\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # expert out 是(b,1,feature_out)\n",
    "        expert_output = torch.concat(\n",
    "            expert_outputs,\n",
    "            dim=1,\n",
    "        )\n",
    "        # expert_output shape (b, num_experts,feature_out)\n",
    "\n",
    "        # expert_weights\n",
    "        expert_weights = F.softmax(expert_weights, dim=1)\n",
    "        # expert_weights shape(b, num_experts)\n",
    "\n",
    "        # (b ,1, num_experts)\n",
    "        expert_weights  =expert_weights.unsqueeze(1)\n",
    "        # (batch,1,feature_out)  希望的输出\n",
    "        output = expert_weights @ expert_output\n",
    "        return output.squeeze(1)\n",
    "\n",
    "def test_basic_moe():\n",
    "    x = torch.rand(4,512)\n",
    "    basic_moe = BasicMOE(512,128,4)\n",
    "    output = basic_moe(x)\n",
    "    print(output.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_basic_moe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMOE(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out,num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(feature_in,num_experts)\n",
    "        # output shape (batch_size, num_export)\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(\n",
    "                feature_in,feature_out\n",
    "                )for _ in range(num_experts)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        # x shape is(batch, feature_in)\n",
    "        expert_weights = self.gate(x)\n",
    "        expert_out_list = [\n",
    "            expert(x) for expert in self.experts\n",
    "        ] # 每一个expert输出一个（batch, feature_out)\n",
    "        \n",
    "        expert_outputs = [\n",
    "            expert_out.unsqueeze(1)\n",
    "            for expert_out in expert_out_list\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        # expert out 是(b,1,feature_out)\n",
    "        expert_output = torch.concat(\n",
    "            expert_outputs,\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # expert out 是(b,1,feature_out)\n",
    "        expert_output = torch.concat(\n",
    "            expert_outputs,\n",
    "            dim=1,\n",
    "        )\n",
    "        # expert_output shape (b, num_experts,feature_out)\n",
    "\n",
    "        # expert_weights\n",
    "        expert_weights = F.softmax(expert_weights, dim=1)\n",
    "        # expert_weights shape(b, num_experts)\n",
    "\n",
    "        # (b ,1, num_experts)\n",
    "        expert_weights  =expert_weights.unsqueeze(1)\n",
    "        # (batch,1,feature_out)  希望的输出\n",
    "        output = expert_weights @ expert_output\n",
    "        return output.squeeze(1)\n",
    "\n",
    "def test_basic_moe():\n",
    "    x = torch.rand(4,512)\n",
    "    basic_moe = BasicMOE(512,128,4)\n",
    "    output = basic_moe(x)\n",
    "    print(output.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_basic_moe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    # 单头注意力机制\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "\n",
    "        # 尝试学习新的写法，attention_mask 通过 register_buffer 注册\n",
    "        # 因为不用计算 梯度，所以节约内存和显存，速度也更快\n",
    "        self.register_buffer(\n",
    "            'attention_mask', \n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            ))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        q = self.query(x)\n",
    "        weight = q @ k.transpose(-2, -1)   # @ 就是 torch.matmul 的简化写法\n",
    "        # 一定要在 softmax 前除以 sqrt(head_size)\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0, \n",
    "            float('-inf')\n",
    "        ) / math.sqrt(hidden_size)  # 这里的 hidden_size 其实是 head_size，因为是单头\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads], \n",
    "            dim=-1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # 实际上就是 MLP\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 接下来就是一个完整的 Block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        head_size = config.n_embd // config.n_head\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 以后会讲  MLA ,  MOE, DPO 完全手写\n",
    "# 完整的  GPT model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # linear (4 -> 8)； weight shape 是记上是 8 * 4，\n",
    "        # 所以 embedding weight 和 lm_head weight 是共享的\n",
    "        # 这里学习一下 tie weight。\n",
    "        # 这是为了减少参数，加快训练；（现在 25的 SLM 很多都这样做了，注意⚠️）\n",
    "        # self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 这里使用的是正态分布初始化\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx 是输入的 token ids\n",
    "        batch, seq_len = idx.size()\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        # seq 长度是这次输入的最大长度\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            # 要确保 位置编码和输入的 idx 在同一个设备上\n",
    "            torch.arange(seq_len, device=idx.device)\n",
    "        )\n",
    "        # 有一个经典题目：为什么 embedding 和 position 可以相加？\n",
    "        x = token_emb + pos_emb   # shape is (batch, seq_len, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)   # shape is (batch, seq_len, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch * seq_len, vocab_size)\n",
    "            targets = targets.view(batch * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列太长，只取最后 block_size 个token\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # 获取预测\n",
    "            logits, _ = self(idx_cond)\n",
    "            # 只关注最后一个时间步的预测\n",
    "            logits = logits[:, -1, :]  # becomes (B, vocab_size)\n",
    "            # 应用softmax获取概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 采样下一个token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # 附加到序列上\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写一个 dataset，为了 Dataloader 准备\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path, block_size=512):\n",
    "        # 我的数据在 /root/fs/mobvoi_seq_monkey_general_open_corpus.jsonl 中，\n",
    "        # 读取前 1000 行\n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        import json\n",
    "\n",
    "        self.encoded_data = []\n",
    "\n",
    "        self.max_lines = 1000\n",
    "        raw_data = []\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= self.max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    text = json.loads(line.strip())['text']\n",
    "                    raw_data.append(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        full_encoded = []\n",
    "        for text in raw_data:\n",
    "            encoded_text = self.enc.encode(text)\n",
    "            full_encoded.extend(encoded_text + [self.eos_token])\n",
    "        \n",
    "        # 将长文本分割成训练样本\n",
    "        for i in range(0, len(full_encoded), self.block_size):\n",
    "            # 多取一个 Token 作为目标\n",
    "            chunk = full_encoded[i:i+self.block_size+1]\n",
    "            # 如果长度不够，用 eos_token 填充\n",
    "            if len(chunk) < self.block_size + 1:\n",
    "                chunk = chunk + [self.eos_token] * (self.block_size + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本编码为token IDs\"\"\"\n",
    "        return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"将token IDs解码为文本\"\"\"\n",
    "        return self.enc.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"text\":\"担任地点省市的区域运营中心的办理作业。承受总部相关KPI查核。\\n1、了解新闻职业或媒体相关运营运营岗位，其间，应聘区域运营中心主任有3年以上当地干流媒体作业经验者优先，应聘事务主管有2年以上当地干流媒体作业经验者优先。\\n2、交流才能强，抗压才能强，长于处理复杂情况，了解GR作业优先，能独立完结策划计划优先。具有独立开发客户才能。\\n北京、天津、河北、山西、黑龙江、吉林、辽宁、上海、江苏、浙江、安徽、江西、福建、山东、河南、湖北、湖南、广东、海南、重庆、四川、贵州、云南、陕西等。\"}\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据的格式\n",
    "\"\"\"\n",
    "{\"text\":\"担任地点省市的区域运营中心的办理作业。承受总部相关KPI查核。\\n1、了解新闻职业或媒体相关运营运营岗位，其间，应聘区域运营中心主任有3年以上当地干流媒体作业经验者优先，应聘事务主管有2年以上当地干流媒体作业经验者优先。\\n2、交流才能强，抗压才能强，长于处理复杂情况，了解GR作业优先，能独立完结策划计划优先。具有独立开发客户才能。\\n北京、天津、河北、山西、黑龙江、吉林、辽宁、上海、江苏、浙江、安徽、江西、福建、山东、河南、湖北、湖南、广东、海南、重庆、四川、贵州、云南、陕西等。\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_dataset = MyDataset('/home/wk/Downloads/dataset/mobvoi_seq_monkey_general_open_corpus.jsonl')\n",
    "\n",
    "# split traindataset to train and val\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 120.116736 M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#model = model.to(device)\n",
    "model = model.to(\"cpu\")\n",
    "# 打印模型一共有多少参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total parameters: {total_params/ 1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "# 设置 cosine 学习率\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 7.75 GiB of which 982.25 MiB is free. Including non-PyTorch memory, this process has 6.66 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 22.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m    train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m    val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, val_loader, device)\n\u001b[1;32m     41\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 调整学习率\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 7.75 GiB of which 982.25 MiB is free. Including non-PyTorch memory, this process has 6.66 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 22.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    " #训练循环\n",
    "def train(model, optimizer, scheduler, train_loader, val_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    model = model.to(device)\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # 将数据移到设备上\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, loss = model(x, targets=y)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 调整学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    return total_loss\n",
    "\n",
    "def eval(model, val_loader, device):\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader, val_loader, device)\n",
    "    val_loss = eval(model, val_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "    }\n",
    "    # 保存每个epoch的模型\n",
    "    torch.save(checkpoint, f'checkpoints/model_epoch_{epoch}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
